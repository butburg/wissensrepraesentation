{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# ML-Fundamentals - Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Requirements](#Requirements) \n",
    "  * [Knowledge](#Knowledge) \n",
    "  * [Modules](#Python-Modules)\n",
    "* [Exercise](#Exercise)\n",
    " * [Data Generation](#Data-Generation)\n",
    " * [Hypothesis](#Hypothesis)\n",
    " * [Plot](#Plot)\n",
    " * [Out of Sample Error](#Out-of-Sample-Error)\n",
    " * [Repeat](#Repeat)\n",
    " * [Average and Plot](#Average-and-Plot)\n",
    " * [Bias](#Bias)\n",
    " * [Variance](#Variance)\n",
    "* [Summary and Outlook](#Summary-and-Outlook)\n",
    "* [Literature](#Literature) \n",
    "* [Licenses](#Licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "If you completed the exercises *simple-linear-regression*, *multivariate-linear-regression* and *logistic-linear-regression* you know how to fit these models according to your training data.\n",
    "\n",
    "This alone so far has no practical use case. The benefit of learning a model is to predict unseen data. Additionally, only with unseen data your model has not learnt from, it is possible to say if your model generalizes well or not. One way to measure this, is calculating the *out of sample error* $E_{out}$, which consists of the measures *bias* and *variance*.\n",
    "\n",
    "In this notebook you will calculate two simple hypothesis for linear regression based on training data and compare them with the use of unseen validation data by calculating $E_{out}$, *bias* and *variance*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "### Knowledge\n",
    "\n",
    "You should have a basic knowledge of:\n",
    "- Univariate linear regression\n",
    "- Out of sample error (bias variance)\n",
    "\n",
    "Suitable sources for acquiring this knowledge are:\n",
    "- [Simple Linear Regression Notebook](http://christianherta.de/lehre/dataScience/machineLearning/basics/univariate-linear-regression.php) by Christian Herta and his [lecture slides](http://christianherta.de/lehre/dataScience/machineLearning/linearRegression.pdf) (German)\n",
    "\n",
    "- [Bias Variance Tradeoff](http://christianherta.de/lehre/dataScience/machineLearning/basics/bias-variance-tradeoff.php) by Christian Herta and his [lecture slides](http://christianherta.de/lehre/dataScience/machineLearning/bias-variance-tradeoff.pdf) (German)\n",
    "\n",
    "- [numpy quickstart](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [Matplotlib tutorials](https://matplotlib.org/tutorials/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Python Modules\n",
    "\n",
    "By [deep.TEACHING](https://www.deep-teaching.org/) convention, all python modules needed to run the notebook are loaded centrally at the beginning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def round_and_hash(value, precision=4, dtype=np.float32):\n",
    "    \"\"\" \n",
    "    Function to round and hash a scalar or numpy array of scalars.\n",
    "    Used to compare results with true solutions without spoiling the solution.\n",
    "    \"\"\"\n",
    "    rounded = np.array([value], dtype=dtype).round(decimals=precision)\n",
    "    hashed = hashlib.md5(rounded).hexdigest()\n",
    "    return hashed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Exercise inspired by lecture 8 from:\n",
    "- Yaser Abu-Mostafa\n",
    " - [Learning from Data, Caltech Machine Learning](http://home.caltech.edu/telecourse.html) [ABU18a]\n",
    " - Or directly at [here](https://www.youtube.com/watch?v=zrEyxfl2-a8&hd=1) [ABU18b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this exercise, the target function $t(x)$ is the sin curve. Our task is to approximate the target using linear regression, i.e. a straight line. To do so we need training samples from the model. A single training sample is a pair of points on the sin curve, generated as follows:\n",
    "* $p(x)$ is the uniform distribution in the interval $[0,2\\pi]$\n",
    "* Two x-values $x^{(1)}$ and $x^{(2)}$ are drawn from $p(x)$\n",
    "* The y-value is the target of x without any additional noise so $y^{(i)}= t(x^{(i)}) = sin(x^{(i)})$\n",
    "\n",
    "![plot1.png](attachment:plot1.png)\n",
    "\n",
    "Using these two points, we'll try two different hypotheses to approximate the sin curve:\n",
    "1. $$\\mathcal H_1: h_1(x) = \\theta_0 + \\theta_1 x$$\n",
    "    A straight line through the two points. The parameters $\\theta_0$ and $\\theta_1$ correspond to the y-intercept and slope of a [linear function](https://www.mathsisfun.com/algebra/linear-equations.html).\n",
    "2. $$\\mathcal H_2: h_2(x) = w$$\n",
    "A constant, i.e. a line parallel to the x-axis. Our choice of $w$ is the mean of the y-values of the two sampled data points.\n",
    "\n",
    "![plot2.png](attachment:plot2.png)\n",
    "\n",
    "To judge which of the hypothesis does a better job of modeling the target, we'll generate more training data and learn parameters for the hypotheses. Then we'll evaluate the hypotheses according to the *our of sample error*, *bias* and *variance*.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Do the following 10.000 times:\n",
    " - Draw two random examples $x^{(1)}$ and $x^{(2)}$ from $p(x)$ and calculate the corresponding $y$s to get $\\mathcal D = \\{(x^{(1)},x^{(2)}), (y^{(1)},y^{(2)})\\}$ *(training data)* \n",
    " - Using your *training data* calculate the parameters $\\theta_0, \\theta_1$ for $\\mathcal H_1$ and the parameter $w$ for $\\mathcal H_2(x)$\n",
    " - Numerically calculate the out of sample error $E_{out}$ for $\\mathcal H_1(x)$ and $\\mathcal H_2(x)$ for 100 data points uniformly distributed in the interval of $[0, 2\\pi]$ (*validation data*) \n",
    " \n",
    " \n",
    "- Now calculate the average $\\theta_0, \\theta_1$ and $w$ of all 10.000 experiments.\n",
    "- Also calculate the average \"out of sample error\" $E_{out}$ for both hypothesis sets $\\mathcal H_1$ and $\\mathcal H_2$.\n",
    "- Use the above to calculate the *bias$^2$* and the *variance*.\n",
    "- Plot the target function $sin(x)$ together with both average hypotheses $\\tilde h_1(x)$ and $\\tilde h_2(x)$ using the average $\\theta_0, \\theta_1$ and $w$\n",
    "- Considering your results, which hypothesis seems to better model the target function?\n",
    "\n",
    "Practically this explanation is all you need to solve the exercise. You are free to complete it without any further guiding or by proceeding with this notebook."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# SKIP/RAW - Code to generate image\n",
    "\n",
    "x_ = np.linspace(0, 2*np.pi)\n",
    "y_  = np.sin(x_)\n",
    "data_x = [0.5, 4]\n",
    "data_y = np.sin(data_x)\n",
    "plt.plot(x_,y_,label='Target function')\n",
    "plt.scatter(data_x, data_y, c='r', marker='x', label='Pair of sampled points')\n",
    "plt.legend();\n",
    "plt.savefig('/home/alyth/Desktop/plot1.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# SKIP/RAW - Code to generate image\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot('111')\n",
    "\n",
    "# plot the sin curve \n",
    "x_ = np.linspace(0, 2*np.pi)\n",
    "y_  = np.sin(x_)\n",
    "ax.plot(x_,y_,label='Target function')\n",
    "\n",
    "# pick two points on the sie curve\n",
    "x1,x2 = [0.5, 4]\n",
    "y1,y2 = np.sin(data_x)\n",
    "ax.scatter([x1,x2],[y1,y2],marker='x',c='r', label='Pair of sampled points')\n",
    "\n",
    "# H1: a line through the two points\n",
    "ax.plot([x1,x2],[y1,y2], label='Hypothesis 1')\n",
    "\n",
    "# H2: a line through the mean y-value\n",
    "w = (y1+y2)/2\n",
    "ax.plot(x_, np.full_like(x_,w), label='Hypothesis 2')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('/home/alyth/Desktop/plot2.png', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "Implement the function to draw two random training examples $(x^{(i)},y^{(i)})$ with:\n",
    "\n",
    "- $x^{(i)} \\in Uniform(0,2\\pi)$\n",
    "- $i \\in \\{1,2\\}$\n",
    "- $y^{(i)} = sin(x^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train_data():\n",
    "    x = np.random.uniform(0, 2*np.pi, 2)\n",
    "    y = np.sin(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train  = train_data() \n",
    "print(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If your implementation is correct, these tests should not throw an exception\n",
    "\n",
    "assert len(x_train) == 2\n",
    "assert len(y_train) == 2\n",
    "np.testing.assert_array_equal(np.sin(x_train), y_train)\n",
    "for i in range(1000):\n",
    "    x_tmp, _ = train_data()\n",
    "    assert x_tmp.min() >= 0.0\n",
    "    assert x_tmp.max() <= 2*np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "For our training data we will now model two different hypothesis sets:\n",
    "\n",
    "$$\\mathcal H_1: h_1(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\mathcal H_2: h_2(x) = w$$\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the functions to calculate the parameters $\\theta_0, \\theta_1$ for $h_1$ and $w$ for $h_2$ using the two drawn examples. \n",
    "\n",
    "For later purpose (passing functions as argument) it is important that both functions accept the same amount of parameters and also return the same amount. Therefore we also pass $x$ to `get_w`, although we do not need it. And for the same reason `get_thetas` should return a list of two values instead of two separate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_thetas(x, y):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def get_w(x, y):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_thetas(x, y):\n",
    "    theta_1 = (y[1] - y[0]) / (x[1] - x[0])\n",
    "    theta_0 = y[0] - x[0] * theta_1\n",
    "    return [theta_0, theta_1]\n",
    "\n",
    "def get_w(x, y):\n",
    "    return y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thetas = get_thetas(x_train, y_train)\n",
    "w = get_w(x_train, y_train)\n",
    "print(thetas[0], thetas[1])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If your implementation is correct, these tests should not throw an exception\n",
    "\n",
    "x_train_temp = np.array([0,1])\n",
    "y_train_temp = np.array([np.sin(x_i) for x_i in x_train_temp])\n",
    "thetas_test = get_thetas(x_train_temp, y_train_temp)\n",
    "w_test = get_w(x_train_temp, y_train_temp)\n",
    "\n",
    "np.testing.assert_almost_equal(thetas_test[0], 0.0)\n",
    "np.testing.assert_almost_equal(thetas_test[1], 0.8414709848078965)\n",
    "np.testing.assert_almost_equal(w_test, 0.42073549240394825)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "Implement the hypothesis $h_1(x)$ and $h_2(x)$. Your function should return a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_hypothesis_1(thetas):\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "def get_hypothesis_2(w):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_hypothesis_1(thetas):\n",
    "    return lambda x: thetas[0] + thetas[1] * x\n",
    "\n",
    "def get_hypothesis_2(w):\n",
    "    print(w.shape)\n",
    "    return lambda x: w * np.ones((len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to compute numerically the expectation w.r.t. x\n",
    "x_grid = np.linspace(0, 2*np.pi, 100)\n",
    "y_grid = np.sin(x_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If your implementation is correct, these tests should not throw an exception\n",
    "\n",
    "h1_test = get_hypothesis_1(thetas_test)\n",
    "h2_test = get_hypothesis_2(w_test)\n",
    "np.testing.assert_almost_equal(h1_test(x_grid)[10], 0.5340523361780719)\n",
    "np.testing.assert_almost_equal(h2_test(x_grid)[10], 0.42073549240394825)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "\n",
    "Following the original exercise it is not yet necessary to plot anything. But it also does not hurt to do so, since we need to implement code for the plot anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "Write the function to plot:\n",
    "- the two examples $(x^{(1)},y^{(2)})$ and $(x^{(2)},y^{(2)})$\n",
    "- the true target function $sin(x)$ in the interval $[0, 2 \\pi]$.\n",
    "- the hypothesis $h_1(x)$ in the interval $[0, 2 \\pi]$\n",
    "- the hypothesis $h_2(x)$ in the interval $[0, 2 \\pi]$\n",
    "    \n",
    "Your plot should look similar to this one:\n",
    "\n",
    "<img src=\"https://gitlab.com/deep.TEACHING/educational-materials/raw/dev/media/klaus/exercise-bias-variance-sin-h1-h2-one-example.png\" width=\"512\" alt=\"internet connection needed\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_true_target_function_x_y_h1_h2(x, y, hypothesis1, hypothesis2):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_true_target_function_x_y_h1_h2(x, y, hypothesis1, hypothesis2):\n",
    "    \n",
    "    x_ = np.linspace(0, 2*np.pi, 100)\n",
    "    y_ = np.sin(x_)\n",
    "    \n",
    "    plt.plot(x_, y_, 'b-', label=\"target sin(x)\")\n",
    "    plt.scatter(x,y,color='r')\n",
    "    \n",
    "    plt.plot(x_, hypothesis1(x_), 'g-', label=\"$h_1(x)$\")\n",
    "    plt.plot(x_, hypothesis2(x_), 'y-', label=\"$h_2(x)$\")\n",
    "    \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train  = train_data() \n",
    "thetas = get_thetas(x_train, y_train)\n",
    "w = get_w(x_train, y_train)\n",
    "plot_true_target_function_x_y_h1_h2(x_train, y_train, get_hypothesis_1(thetas), get_hypothesis_2(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Sample Error\n",
    "\n",
    "The _out of sample error_ $E_{out}(h)$ is the expected error on new unseen data.\n",
    "\n",
    "$$\n",
    "E_{out}(h) = \\mathbb E_{x,y}[loss(h(x), y)]  = \\int_{\\mathcal X \\times \\mathcal Y} loss(h(x), y) p(x,y) dx dy\n",
    "$$\n",
    "\n",
    "\n",
    "In our example we don't need to take the expectation w.r.t. $y$ because we have have no noise:\n",
    "\n",
    "$$\n",
    "E_{out}(h) = \\mathbb E_{x}[loss(h(x), t(x))]  = \\int_{\\mathcal X } loss(h(x), t(x)) p(x) dx\n",
    "$$\n",
    "\n",
    "Here we will compute the _out of sample error_ $E_{out}(h)$ numerically.      \n",
    "     \n",
    "We already have discretized the $x$-axis (`x_grid`) for $p(x)\\neq 0$.      \n",
    "So to compute the expectation we just need to average over `x_grid` (remember $p(x)$ is uniform).      \n",
    "\n",
    "$$\n",
    "E_{out}(h) = \\mathbb E_{x}[loss(h(x),t(x))] \\approx \\frac{1}{m} \\sum_{j=1}^{m} loss(h(x^{(j)}), y^{(j)}) \n",
    "$$\n",
    "\n",
    "with\n",
    "- $m$: number of elements in `x_grid`\n",
    "- $x^{(j)}$ is the $j$-element of `x_grid`\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the function to numerically calculate the out of sample error $E_{out}$ with the mean squared error as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def out_of_sample_error(y_preds, y):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def out_of_sample_error(y_preds, y):\n",
    "    return ((y_preds - y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If your implementation is correct, these tests should not throw an exception\n",
    "\n",
    "e_out_h1_test = out_of_sample_error(h1_test(x_grid), y_grid)\n",
    "np.testing.assert_almost_equal(e_out_h1_test, 11.525485917588728)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Now instead of drawing two examples (one training data set), draw \n",
    "now 10.000 times different training sets with two examples.    \n",
    "Calculate $E_{out}$ for the different $h_1$ and $h_2$ numerically.\n",
    "\n",
    "For each run, keep track of the following parameters and return them at the end of the function:\n",
    "- $\\{x^{(1)},x^{(2)}\\}$\n",
    "- $\\{y^{(1)},y^{(2)}\\}$\n",
    "- $\\theta_0$\n",
    "- $\\theta_1$\n",
    "- $w$\n",
    "- $E_{out}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(m):\n",
    "    xs = np.ndarray((m,2))\n",
    "    ys = np.ndarray((m,2))\n",
    "    t0s = np.ndarray(m)\n",
    "    t1s = np.ndarray(m)\n",
    "    ws = np.ndarray(m)\n",
    "    e_out_h1s = np.ndarray(m)\n",
    "    e_out_h2s = np.ndarray(m)\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return xs, ys, t0s, t1s, ws, e_out_h1s, e_out_h2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(m, x_grid, y_grid):\n",
    "    xs = np.ndarray((m,2))\n",
    "    ys = np.ndarray((m,2))\n",
    "    t0s = np.ndarray(m)\n",
    "    t1s = np.ndarray(m)\n",
    "    ws = np.ndarray(m)\n",
    "    e_out_h1s = np.ndarray(m)\n",
    "    e_out_h2s = np.ndarray(m)\n",
    "    \n",
    "    for i in range(m):\n",
    "        x, y = train_data()\n",
    "        thetas = get_thetas(x,y)\n",
    "        w = get_w(x, y)\n",
    "        \n",
    "        xs[i] = x\n",
    "        ys[i] = y\n",
    "        t0s[i] = thetas[0]\n",
    "        t1s[i] = thetas[1]\n",
    "        ws[i] = w\n",
    "        \n",
    "        h1 = get_hypothesis_1(thetas)\n",
    "        h2 = get_hypothesis_2(w)\n",
    "        \n",
    "        e_out_h1s[i] = out_of_sample_error(h1(x_grid), y_grid)\n",
    "        e_out_h2s[i] = out_of_sample_error(h2(x_grid), y_grid)\n",
    "        \n",
    "    return xs, ys, t0s, t1s, ws, e_out_h1s, e_out_h2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs, ys, t0s, t1s, ws, e_out_h1s, e_out_h2s = run_experiment(\n",
    "    10000, x_grid, y_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average and Plot\n",
    "\n",
    "\n",
    "Now we can calculate the average of $\\theta_0, \\theta_1$, $w$ and $E_{out}$ and already plot the resulting averaged $\\tilde h_1(x)$ and $\\tilde h_2(x)$ together with the target function $sin(x)$.\n",
    "\n",
    "Your plot should look similar to the one below:\n",
    "\n",
    "<img src=\"https://gitlab.com/deep.TEACHING/educational-materials/raw/dev/media/klaus/exercise-bias-variance-sin-h1-h2-avg.png\" width=\"512\" alt=\"internet connection needed\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "t0_avg = t0s.mean()\n",
    "t1_avg = t1s.mean()\n",
    "thetas_avg = [t0_avg, t1_avg]\n",
    "w_avg = ws.mean()\n",
    "h1_avg = get_hypothesis_1(thetas_avg)\n",
    "h2_avg = get_hypothesis_2(w_avg)\n",
    "print(thetas_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_true_target_function_x_y_h1_h2([], [], h1_avg, h2_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "expectation_Eout_1 = e_out_h1s.mean()\n",
    "print (\"expectation of E_out of model 1:\", expectation_Eout_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "expectation_Eout_2 = e_out_h2s.mean()\n",
    "print (\"expectation of E_out of model 2:\", expectation_Eout_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "The bias for the mean-squared error is:\n",
    "$$\n",
    "bias^2 = \\mathbb E_{x,y} \\left[(\\tilde h(x) - y)^2\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "The expectation w.r.t. $y$ vanishes if we have no noise:\n",
    "\n",
    "$$\n",
    "bias^2 = \\mathbb E_x \\left[(\\tilde h(x) - t(x))^2\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "with:\n",
    "\n",
    "- the average hypothesis $\\tilde h(x)$\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the function to calculate the expecation of the bias numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def bias(y_true, y_predicted):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def bias(y_true, y_predicted):\n",
    "    return ((y_true - y_predicted)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bias_1 = bias(y_grid,  h1_avg(x_grid))\n",
    "print (\"Bias of model 1:\", bias_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bias_2 = bias(y_grid,  h2_avg(x_grid))\n",
    "print (\"Bias of model 2:\", bias_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "Variance for the mean-squared-error:\n",
    "\n",
    "$$\n",
    "variance = \\mathbb E_x \\left[ \\mathbb E_D \\left[\\left(h^D(x) - \\tilde h(x)\\right)^2 \\right] \\right]\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "- the average hypothesis $\\tilde h(x)$\n",
    "- the learned hypothesis $h^D(x)$ for training data set $D$.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the function to calculate the variances for each of the 10.000 experiments and return them as list or array.\n",
    "\n",
    "Now we benefit from our implementation of `get_w`, `get_thetas`, respectively `get_hypothesis1`,`get_hypothesis2`, which accept and return the same amount of parameters, so we can write a generalized function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def variances(hypothesis_func, param_func, xs, ys, x_val, y_preds):\n",
    "    return NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def variances(hypothesis_func, param_func, xs, ys, x_val, y_preds):\n",
    "    # y_preds is the prediction of the average hypothesis\n",
    "    \n",
    "    # variance\n",
    "    deltas_m = np.ndarray(len(xs))\n",
    "    \n",
    "    for i in range(len(xs)): # loop over different training data \n",
    "        params = param_func(xs[i], ys[i])\n",
    "        h_temp = hypothesis_func(params)\n",
    "        y_pred_temp = h_temp(x_val)\n",
    "        deltas_m[i] = ((y_pred_temp - y_preds)**2).mean()\n",
    "    \n",
    "    return deltas_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vars_1 = variances(get_hypothesis_1, \n",
    "                 get_thetas, \n",
    "                 xs, ys, \n",
    "                 x_grid, \n",
    "                 h1_avg(x_grid))\n",
    "var_1_avg = vars_1.mean()\n",
    "print(var_1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vars_2 = variances(get_hypothesis_2, \n",
    "                 get_w, \n",
    "                 xs, ys, \n",
    "                 x_grid, \n",
    "                 h2_avg(x_grid))\n",
    "var_2_avg = vars_2.mean()\n",
    "print(var_2_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"model 1: E_out ≈ bias^2 + variance:  %f ≈ %f + %f\" % (expectation_Eout_1, bias_1, var_1_avg))\n",
    "print(\"model 2: E_out ≈ bias^2 + variance:  %f ≈ %f + %f\" % (expectation_Eout_2, bias_2, var_2_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Summary and Outlook\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "Exercise: Bias Variance Tradeoff <br/>\n",
    "by Christian Herta, Klaus Strohmenger<br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Christian Herta, Klaus Strohmenger\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
