{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML-Fundamentals - Linear Regression - Exercise: Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Requirements](#Requirements) \n",
    "  * [Knowledge](#Knowledge) \n",
    "  * [Modules](#Python-Modules)\n",
    "* [Exercises: Simple Linear Regression](#Simple-Linear-Regression)\n",
    "  * [Create a Data Set](#Create-a-Data-Set)\n",
    "  * [Linear Hypothesis](#Linear-Hypothesis)\n",
    "  * [Cost Function](#Cost-Function)\n",
    "  * [Cost Function Visualization](#Cost-Function-Visualization)\n",
    "  * [Optimizing wit Gradient Descent](#Gradient-Descent)\n",
    "  * [Model Evaluation](#Model-and-Training-Evaluation)\n",
    "  * [Optimize Hyperparameters](#Optimize-Hyperparameter)  \n",
    "* [Summary and Outlook](#Summary-and-Outlook) \n",
    "* [Literature](#Literature) \n",
    "* [Licenses](#Licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "Linear Regression is the _Hello World_ of Machine Learning. In this exercise you will implement a _simple linear regression (univariate linear regression)_, a model with one predictor and one response variable. The goal is to recap and practice fundamental concepts of Machine Learning. After the exercise, you should have a deeper understanding of what a Machine Learning model is and how do you train such a model with a data set (supervised learning). To achieve this, you will:\n",
    "1. Calculate the cost and the gradient for concrete training data and $\\theta$ (pen & paper exercise)\n",
    "2. Create your own data set\n",
    "3. Implement a linear function as hypothesis (model) \n",
    "4. Write a function to quantify your model (cost function)\n",
    "5. Learn to visualize the cost function\n",
    "6. Implement the gradient descent algorithm to train your model (optimizer) \n",
    "7. Visualize your training process and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Requirements\n",
    "### Knowledge\n",
    "You should have a basic knowledge of Machine Learning models, cost functions, optimization algorithms and also numpy and matplotlib. We will only recap these concepts for a better understanding and do not explain them in great detail. Suitable sources for acquiring this knowledge are:\n",
    "- [Simple Linear Regression Notebook](http://christianherta.de/lehre/dataScience/machineLearning/basics/univariate-linear-regression.php) by Christian Herta and his [lecture slides](http://christianherta.de/lehre/dataScience/machineLearning/linearRegression.pdf) (German)\n",
    "- Chapter 2 of the open classroom [Machine Learning](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning) by Andrew Ng\n",
    "- Chapter 5.1 of [Deep Learning](http://www.deeplearningbook.org/contents/ml.html) by Ian Goodfellow \n",
    "- Some parts of chapter 1 and 3 of [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book) by Christopher M. Bishop\n",
    "- [numpy quickstart](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [Matplotlib tutorials](https://matplotlib.org/tutorials/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Modules\n",
    "\n",
    "By [deep.TEACHING](https://www.deep-teaching.org/) convention, all python modules needed to run the notebook are loaded centrally at the beginning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# External Modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises - Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pen & Paper Exercise\n",
    "\n",
    "Given the linear model:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "\n",
    "And the following concrete training data:\n",
    "\n",
    "$$\n",
    "D_{train} = \\{(0,1),(1,3),(2,6),(4,8)\\}\n",
    "$$\n",
    "\n",
    "with each tuple $(x,y)$ denoting $x$ the feature and $y$ the target.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "For $\\theta_0 = 1$ and $\\theta_1 = 2$ calculate:\n",
    "\n",
    "1. The cost:\n",
    "\n",
    "$$ J_D(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}{(h_\\theta(x^{(i)})-y^{(i)})^2} $$\n",
    "\n",
    "2. The gradient $\\nabla J$, i.e. the partial derivatives:\n",
    "\n",
    "$$ \\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_0} $$\n",
    "\n",
    "$$ \\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_1} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "Just put $D_{train} = \\{(0,1),(1,3),(2,6),(4,8)\\}$ into the equations, so we get:\n",
    "\n",
    "1. \n",
    "For the cost:\n",
    "$$\n",
    "J_D(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}{(h_\\theta(x^{(i)})-y^{(i)})^2} = \\frac{1}{2\\cdot 4} (0 + 0 + 1^2 + (- 1)^2) = \\frac{1}{4}\n",
    "$$\n",
    "\n",
    "2. \n",
    "For $ \\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_0} $ we have:\n",
    "$$\n",
    "\\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_0} = \\ldots =  \\frac{1}{m}\\sum_{i=1}^{m}(\\theta_0 + \\theta_1 x^i - y^i) = \\frac{1}{m} (0 + 0 + 1 + (- 1)) = 0\n",
    "$$\n",
    " For $ \\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_1} $ we have:\n",
    "$$\n",
    "\\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_0} = \\ldots = \\frac{1}{m}\\sum_{i=1}^{m}((\\theta_0 + \\theta_1 x^i - y^i) \\cdot x^i) = \\frac{1}{4} (0 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 2 + (- 1) \\cdot 4) = \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create a Data Set\n",
    "First of all, you have to generate a data set $\\mathcal D_{train}$. $\\mathcal D_{train}$ consists of tuples $(x^{(i)},y^{(i)})$. Let $x$ and $y$ be two numpy 1d-arrays of equal-length $m$:\n",
    "\n",
    "$$\n",
    "{\\vec x} = \\left(x^{(1)},x^{(2)}, \\ldots, x^{(m)}\\right)^T \\\\\n",
    "{\\vec y} = \\left(y^{(1)},y^{(2)}, \\ldots, y^{(m)}\\right)^T\n",
    "$$\n",
    "\n",
    "The $x$ values should be drawn from a **uniform distribution** . Add some noise $\\delta$ to the corresponding $y$ values, which should be drawn from a **normal distribution**.\n",
    "\n",
    "$$\n",
    "    y^{(i)} = a + b * x^{(i)} + \\delta^{(i)}\n",
    "$$\n",
    "\n",
    "We will use an numpy 1d-array for the data which is equivalent to vectors: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\vec y = a * \\vec 1 + b * \\vec x + \\vec \\delta\n",
    "\\end{equation}\n",
    "\n",
    ", with:\n",
    "* $x \\sim Uniform([x_{min}, x_{max}])$\n",
    "* $\\delta \\sim Normal(\\mu=0.0, \\sigma=1.0)$\n",
    "\n",
    "An example data set could look like this plot (random seed 42 used): \n",
    "\n",
    "<img src=\"https://gitlab.com/deep.TEACHING/educational-materials/raw/dev/media/voigt/images/Simple-Linear-Regression_Data_Set.png\" width=\"512\" alt=\"internet connection needed\">\n",
    "\n",
    "**Hint:**\n",
    "- To generate the vector for $x$ use the function `np.random.uniform`.\n",
    "- To generate a vector of the noise use `np.random.randn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# fixed random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "def linear_random_data(sample_size, a, b, x_min, x_max, noise_factor):\n",
    "    '''creates a randam data set based on a linear function in a given interval\n",
    "    \n",
    "    Args:\n",
    "        sample_size: number of data points\n",
    "        a: coefficent of x^0\n",
    "        b: coefficent of x^1\n",
    "        x_min: lower bound value range\n",
    "        x_max: upper bound value range\n",
    "        noise_factor: strength of noise added to y \n",
    "    \n",
    "    Returns:\n",
    "        x: array of x values | len(x)==len(y)\n",
    "        y: array of y values corresponding to x | len(x)==len(y)\n",
    "    '''\n",
    "    raise NotImplementedError(\"You should implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution marked as 'skip' slide\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def linear_random_data(sample_size, a, b, x_min, x_max, noise_factor):\n",
    "    '''creates a random data set based on a lienar function in a given interval\n",
    "    \n",
    "    Args:\n",
    "        sample_size: number of data points\n",
    "        a: coefficent of x^0\n",
    "        b: coefficent of x^1\n",
    "        x_min: lower bound value range\n",
    "        x_max: upper bound value range\n",
    "        noise_factor: strength of nosie added to y \n",
    "    \n",
    "    Returns:\n",
    "        x: array of x values | len(x)==len(y)\n",
    "        y: array of y values corresponding to x | len(x)==len(y)\n",
    "    '''\n",
    "    x = np.random.uniform(x_min, x_max, sample_size)\n",
    "    y = a + b * x + np.random.randn(sample_size) * noise_factor\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3e204e13c8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = linear_random_data(sample_size=50, a=0., b=5., x_min=-10, x_max=10, noise_factor=5)\n",
    "plt.plot(x,y, \"rx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Hypothesis\n",
    "A short recap, a hypothesis $h_\\theta(x)$ is a certain function that we believe is similar to a target function that we like to model. A hypothesis $h_\\theta(x)$ is a function of $x$ with fixed parameters $\\theta$. The simplest kind of hypothesis is based on a linear equation with two parameters: \n",
    "\n",
    "\\begin{equation}\n",
    "    h_\\theta(x) = \\theta_{0} + \\theta_{1} * x \n",
    "\\end{equation}\n",
    "\n",
    "Implement hypothesis $h_\\theta(x)$ in the method `linear_hypothesis` and return it as a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def linear_hypothesis(theta_0, theta_1):\n",
    "    ''' Combines given arguments in a linear equation and returns it as a function\n",
    "    \n",
    "    Args:\n",
    "        theta_0: first coefficient\n",
    "        theta_1: second coefficient\n",
    "        \n",
    "    Returns:\n",
    "        lambda that models a linear function based on theta_0, theta_1 and x\n",
    "    ''' \n",
    "    raise NotImplementedError(\"You should implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def linear_hypothesis(theta_0, theta_1):\n",
    "    ''' Combines given arguments in a linear equation and returns it as a function\n",
    "    \n",
    "    Args:\n",
    "        theta_0: first coefficient\n",
    "        theta_1: second coefficient\n",
    "        \n",
    "    Returns:\n",
    "        lambda that models a linear function based on theta_0, theta_1 and x\n",
    "    '''\n",
    "    return lambda x: theta_0 + theta_1 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cost Function\n",
    "A cost function $J$ depends on the given training data $D$ and hypothesis $h_\\theta(x)$. In the context of the simple linear regression, the cost function measures how wrong a model is regarding its ability to estimate the relationship between $x$ and $y$ for specific $\\theta$ values. Later we will treat this as an optimization problem and try to minimize the cost function $J_D(\\theta)$ to find optimal $\\theta$ values for our hypothesis $h_\\theta(x)$. The cost function we use in this exercise is the [Mean-Squared-Error](https://en.wikipedia.org/wiki/Mean_squared_error) cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "    J_D(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}{\\left(h_\\theta(x^{(i)}-y^{(i)}\\right)^2}\n",
    "\\end{equation}\n",
    "\n",
    "Implement the cost function $J_D(\\theta)$ in the method `mse_cost_function`. The method should return a function that takes the values of $\\theta_0$ and $\\theta_1$ as an argument.\n",
    "\n",
    "Sidenote, the terms loss function or error function are often used interchangeably in the field of Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def mse_cost_function(x, y):\n",
    "    ''' Implements MSE cost function as a function J(theta_0, theta_1) on given tranings data \n",
    "    \n",
    "    Args:\n",
    "        x: vector of x values \n",
    "        y: vector of ground truth values y \n",
    "        \n",
    "    Returns:\n",
    "        lambda J(theta_0, theta_1) that models the cost function\n",
    "    '''\n",
    "    raise NotImplementedError(\"You should implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mse_cost_function(x, y):\n",
    "    ''' Implements MSE cost function as a function J(theta_0, theta_1) on given tranings data \n",
    "    \n",
    "    Args:\n",
    "        x: vector of x values \n",
    "        y: vector of ground truth values y \n",
    "        \n",
    "    Returns:\n",
    "        lambda J(theta_0, theta_1) that models the cost function\n",
    "    '''\n",
    "    assert(len(x) == len(y))\n",
    "    m = len(x)\n",
    "    return lambda theta_0, theta_1: 1./(2. * float(m)) * np.sum((linear_hypothesis(theta_0, theta_1)(x) - y )**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.2540351858\n",
      "13.2554306346\n"
     ]
    }
   ],
   "source": [
    "j = mse_cost_function(x, y)\n",
    "print(j(2.1, 2.9))\n",
    "print(j(2.3, 4.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Suggestion (from Diyar)**\n",
    "\n",
    "The contour/3D plots take a while to grasp. I feel at this point you may want to start off with a more simple visualization: plot the data set and a single hypothesis. Then encourage the reader to try different hypotheses. The aim is to:\n",
    "* show how tweaks of $\\theta_0$ and $\\theta_1$ move the hypothesis\n",
    "* show the effect of choosing parameters that are close/far from the true model on how close/far the hypothesis is to the scattered data set\n",
    "* get comfortable with the syntax/usage of closures for hypothesis and cost in this particular implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Visualisation of Hypotheses\n",
    "\n",
    "Now we want to visualize the data set together with the hypothesis and explore the effects of adjusting the parameters. In your plot,\n",
    "* show your data set as a scatter plot\n",
    "* show a hypothesis h(x) as a line plot\n",
    "* calculate and present the MSE cost (a print statement will do as well)\n",
    "\n",
    "Here are some questions you may want to address:\n",
    "* How does the line modeling the hypothesis change when you tweak $\\theta_0$?\n",
    "* How does the line modeling the hypothesis hypothesis change when you tweak $\\theta_1$?\n",
    "* Try some different hypotheses with parameters that are close to the true model and some that are far from the truth. How does this affect the location of the hypothesis in relation to the data?\n",
    "* What is the effect on the cost?\n",
    "\n",
    "Your plot could look similar to the following:\n",
    "![download.png](attachment:download.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_data_with_hypothesis(x, y, theta0, theta1):\n",
    "    ''' Plots the data (x, y) together with a hypothesis given theta0 and theta1.    \n",
    "    '''\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data_with_hypothesis(x, y, theta0, theta1):\n",
    "    ''' Plots the data (x, y) together with a hypothesis given theta0 and theta1.    \n",
    "    '''\n",
    "    # parameters (true values are 0,5)\n",
    "    t0,t1 = theta0, theta1\n",
    "\n",
    "    # plot the data points\n",
    "    plt.plot(x,y,'rx',label='data set')\n",
    "\n",
    "    # plot the hypothesis\n",
    "    x_ = np.arange(-10,10)\n",
    "    y_ = t0 + t1 * x_\n",
    "    plt.plot(x_,y_, 'b-', label='hypothesis')\n",
    "\n",
    "    # cost\n",
    "    cost = mse_cost_function(x,y)(t0,t1)\n",
    "    plt.text(0,-40,'$\\\\theta_0$ : {} \\n$\\\\theta_1$ : {} \\nCost : {}'.format(t0,t1,cost))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # * t0 moves the hypothesis along y-axis/sets y-offset\n",
    "    # * t1 changes the gradient of the hypothesis\n",
    "    # * when t0 and t1 are close to the true model, the line is close to the\n",
    "    # data points and the cost is lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhAAAAFkCAYAAABxWwLDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3dff376160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t0 = 1 ### try different values\n",
    "t1 = 1 ### try different values\n",
    "\n",
    "plot_data_with_hypothesis(x, y, theta0=t0, theta1=t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cost Function Visualization\n",
    "\n",
    "After implementing a cost function, it is probably a good idea to visualize it to get from an abstract understanding to a more concrete representation. Use `matplotlib` and plot the cost function in two different ways. Create a [contour plot](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contour.html) that depicts a three-dimensional surface on a two-dimensional graph and plot the [surface](https://matplotlib.org/tutorials/toolkits/mplot3d.html?highlight=3d%20surface#mpl_toolkits.mplot3d.Axes3D.plot_surface) itself. Your visualization should consist of two subplots and have corresponding labeling, similar to the following example:\n",
    "\n",
    "![internet connection needed](https://gitlab.com/deep.TEACHING/educational-materials/raw/dev/media/voigt/images/Simple-Linear-Regression_Cost-Function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def create_cost_plt_data(cost_func, interval, num_samples, x_offset=0., y_offset=0.):\n",
    "    ''' Creates data for a 3D plot based on a given interval and a cost function\n",
    "    \n",
    "    The function creates two vectors t0 and t1 based on the 'interval' and number \n",
    "    of data point 'num_samples'. Using 'np.meshgrid' and the vectors two matrices \n",
    "    T0 and T1 are created that can be used as X  and Y during the plotting process. \n",
    "    Using the given 'cost_func' and the vector an additional matrix 'C ' is \n",
    "    created representing the cost for all data point combinations.\n",
    "    \n",
    "    Args:\n",
    "        cost_func: a function that is used to calculate costs C \n",
    "        interval: a scalar that defines the range [-interval,interval] data points \n",
    "                  are drawn from\n",
    "        num_samples: number of data points drawn from the interval, equaly distributed \n",
    "        x_offset: shifts the interval by a scalar\n",
    "        y_offset: shifts the interval by a scalar\n",
    "        \n",
    "    Returns:\n",
    "        T0: a matrix representing a meshgrid for X values (Theta 0) \n",
    "        T1: a matrix representing a meshgrid for Y values (Theta 1)\n",
    "        C: a matrix respresenting cost values        \n",
    "    '''\n",
    "    raise NotImplementedError(\"You should implement this!\")\n",
    "\n",
    "def create_cost_plt(T0, T1, Costs):\n",
    "    ''' Creates a counter and a surface plot based on given data\n",
    "    \n",
    "    Args:\n",
    "        T0: a matrix representing a meshgrid for X values (Theta 0) \n",
    "        T1: a matrix representing a meshgrid for Y values (Theta 1)\n",
    "        C: a matrix respresenting cost values \n",
    "    '''\n",
    "    raise NotImplementedError(\"You should implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def create_cost_plt_data(cost_func, interval, num_samples, x_offset=0., y_offset=0.):\n",
    "    ''' Creates data for a 3D plot based on a given interval and a cost function\n",
    "    \n",
    "    The function creates two vectors t0 and t1 based on the 'interval' and number \n",
    "    of data point 'num_samples'. Using 'np.meshgrid' and the vectors two matrices \n",
    "    T0 and T1 are created that can be used as X  and Y during the plotting process. \n",
    "    Using the given 'cost_func' and the vector an additional matrix 'C ' is \n",
    "    created representing the cost for all data point combinations.\n",
    "    \n",
    "    Args:\n",
    "        cost_func: a function that is used to calculate costs C \n",
    "        interval: a scalar that defines the range [-interval,interval] data points \n",
    "                  are drawn from\n",
    "        num_samples: number of data points drawn from the interval, equaly distributed \n",
    "        x_offset: shifts the interval by a scalar\n",
    "        y_offset: shifts the interval by a scalar\n",
    "        \n",
    "    Returns:\n",
    "        T0: a matrix representing a meshgrid for X values (Theta 0) \n",
    "        T1: a matrix representing a meshgrid for Y values (Theta 1)\n",
    "        C: a matrix respresenting cost values        \n",
    "    '''\n",
    "    # initilize grid by given arguments and create a meshgrid\n",
    "    t0 = np.linspace(-interval+x_offset, interval+x_offset, num=num_samples)\n",
    "    t1 = np.linspace(-interval+y_offset, interval+y_offset, num=num_samples)\n",
    "    T0, T1 = np.meshgrid(t0, t1) \n",
    "    \n",
    "    # initilize cost matrix and cost function\n",
    "    C = np.zeros([len(t0),len(t1)])\n",
    "    c = cost_func(x, y)\n",
    "    ### ucost_func = np.frompyfunc(cost_func, 2, 1)\n",
    "\n",
    "    # calculate cost for each pair of the grid\n",
    "    for i, t_0 in enumerate(t0):\n",
    "        for j, t_1 in enumerate(t1):\n",
    "            C[j][i] = c(t_0, t_1)\n",
    "            \n",
    "    return T0, T1, C\n",
    "\n",
    "def create_cost_plt(T0, T1, Costs):\n",
    "    ''' Creates a counter and a surface plot based on given data\n",
    "    \n",
    "    Args:\n",
    "        T0: a matrix representing a meshgrid for X values (Theta 0) \n",
    "        T1: a matrix representing a meshgrid for Y values (Theta 1)\n",
    "        C: a matrix respresenting cost values \n",
    "    '''\n",
    "    # contour plot\n",
    "    fig = plt.figure(figsize=(13,6))\n",
    "    fig.suptitle('Visualization of MSE Cost Function')\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.contour(T0, T1, C)\n",
    "    ax.set_xlabel('Theta 0')\n",
    "    ax.set_ylabel('Theta 1')\n",
    "\n",
    "    # surface plot\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    surf = ax.plot_surface(T0, T1, C, rstride=1, cstride=1, cmap='viridis', linewidth=0, antialiased=False)\n",
    "    ax.set_xlabel('Theta 0', labelpad=9)\n",
    "    ax.set_ylabel('Theta 1', labelpad=9)\n",
    "    ax.set_zlabel('Costs', labelpad=9)\n",
    "    fig.colorbar(surf, shrink=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create some data and plot it\n",
    "T0, T1, C = create_cost_plt_data(mse_cost_function, 1000, 51, y_offset=5.)\n",
    "create_cost_plt(T0, T1, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Gradient Descent\n",
    "\n",
    "A short recap, the gradient descent algorithm is a first-order iterative optimization for finding a minimum of a function. From the current position in a (cost) function, the algorithm steps proportional to the negative of the gradient and repeats this until it reaches a local or global minimum and determines. Stepping proportional means that it does not go entirely in the direction of the negative gradient, but scaled by a fixed value $\\alpha$ also called the learning rate. Implementing the following formalized update rule is the core of the optimization process:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\theta_{j}^{new} \\leftarrow \\theta_{j}^{new} - \\alpha * \\frac{\\partial}{\\partial\\theta_{j}} J(\\theta^{old})\n",
    "\\end{equation}\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\theta_{j_{old}}} J(\\theta_{old})$ is the partial derivative (gradient) of the cost function with respect to the j-th parameter.\n",
    "\n",
    "Partial derivative for $\\theta_0$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_0} J(\\Theta)\n",
    "= \\frac{1}{m} \\sum_{i=1}^m (\\theta_0 + \\theta_1 \\cdot x^{(i)} - y^{(i)})\n",
    "$$\n",
    "\n",
    "Partial derivative for $\\theta_1$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^m (\\theta_0 + \\theta_1 \\cdot x^{(i)} - y^{(i)}) \\cdot x^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def update_theta(x, y, theta_0, theta_1, learning_rate):\n",
    "    ''' Updates learnable parameters theta_0 and theta_1 \n",
    "    \n",
    "    The update is done by calculating the partial derivities of \n",
    "    the cost function including the linear hypothesis. The \n",
    "    gradients scaled by a scalar are subtracted from the given \n",
    "    theta values.\n",
    "    \n",
    "    Args:\n",
    "        x: array of x values\n",
    "        y: array of y values corresponding to x\n",
    "        theta_0: current theta_0 value\n",
    "        theta_1: current theta_1 value\n",
    "        learning_rate: value to scale the negative gradient \n",
    "        \n",
    "    Returns:\n",
    "        t0: Updated theta_0\n",
    "        t1: Updated theta_1\n",
    "    '''\n",
    "    raise NotImplementedError(\"You should implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def update_theta(x, y, theta_0, theta_1, learning_rate):\n",
    "    ''' Updates learnable parameters theta_0 and theta_1 \n",
    "    \n",
    "    The update is done by calculating the partial derivities of \n",
    "    the cost function including the linear hypothesis. The \n",
    "    gradients scaled by a scalar are subtracted from the given \n",
    "    theta values.\n",
    "    \n",
    "    Args:\n",
    "        x: array of x values\n",
    "        y: array of y values corresponding to x\n",
    "        theta_0: current theta_0 value\n",
    "        theta_1: current theta_1 value\n",
    "        learning_rate: value to scale the negative gradient \n",
    "        \n",
    "    Returns:\n",
    "        t0: Updated theta_0\n",
    "        t1: Updated theta_1\n",
    "    '''\n",
    "    m = len(x)\n",
    "    # calculate gradients\n",
    "    dt0 = 1./float(m) * (theta_0 + theta_1 * x - y).sum()  \n",
    "    dt1 = 1./float(m) * ((theta_0 + theta_1 * x - y) * x ).sum() \n",
    "    # update theta values\n",
    "    t0 = theta_0  - learning_rate * dt0\n",
    "    t1 = theta_1  - learning_rate * dt1\n",
    "    return t0, t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using the `update_theta` method, you can now implement the gradient descent algorithm. Iterate over the update rule to find a $\\theta_0$ and a $\\theta_1$ that minimize our cost function $J_D(\\theta)$. This process is often called training of a machine learning model. During the training process create a history of all theta and cost values. You can use them later for evaluation. Implement a `verbose` argument that if true provides additional information during the process, e.g., final theta values after optimization or cost value at some iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, iterations=1000, learning_rate=0.0001, verbose=False):\n",
    "    ''' Minimize theta values of a linear model based on MSE cost function\n",
    "    \n",
    "    Args:\n",
    "        x: vector, x values from the data set\n",
    "        y: vector, y values from the data set\n",
    "        iterations: scalar, number of theta updates\n",
    "        learning_rate: scalar, scales the negative gradient \n",
    "        verbose: boolean, print addition information \n",
    "        \n",
    "    Returns:\n",
    "        t0s: list of theta_0 values, one value for each iteration\n",
    "        t1s: list of theta_1 values, one value for each iteration\n",
    "        costs: list oft costs, one value for each iteration.\n",
    "    '''\n",
    "    raise NotImplementedError(\"You should implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, iterations=1000, learning_rate=0.0001, verbose=False):\n",
    "    ''' Minimize theta values of a linear model based on MSE cost function\n",
    "    \n",
    "    Args:\n",
    "        x: vector, x values from the data set\n",
    "        y: vector, y values from the data set\n",
    "        iterations: scalar, number of theta updates\n",
    "        learning_rate: scalar, scales the negative gradient \n",
    "        verbose: boolean, print addition information \n",
    "        \n",
    "    Returns:\n",
    "        t0s: list of theta_0 values, one value for each iteration\n",
    "        t1s: list of theta_1 values, one value for each iteration\n",
    "        costs: list oft costs, one value for each iteration.\n",
    "    '''\n",
    "    # initilize histories\n",
    "    cost_hist = np.zeros(iterations)\n",
    "    t0_hist = np.zeros(iterations)\n",
    "    t1_hist = np.zeros(iterations)\n",
    "\n",
    "    # initialize first theta values \n",
    "    theta_0 = np.random.random()\n",
    "    theta_1 = np.random.random()\n",
    "    \n",
    "    # get loss function based on given data\n",
    "    cost = mse_cost_function(x, y)\n",
    "    \n",
    "    t0_hist[0] = theta_0\n",
    "    t1_hist[0] = theta_1\n",
    "    cost_hist[0] = cost(theta_0, theta_1)\n",
    "    # optimize theta values\n",
    "    for i in range(iterations):\n",
    "        theta_0, theta_1 = update_theta(x, y, theta_0, theta_1, learning_rate)\n",
    "        t0_hist[i] = theta_0\n",
    "        t1_hist[i] = theta_1\n",
    "        cost_hist[i] = cost(theta_0, theta_1)\n",
    "    if verbose:    \n",
    "        print('costs:', cost_hist[-1])\n",
    "        print('theta_0:', t0_hist[-1])\n",
    "        print('theta_1:', t1_hist[-1])\n",
    "    return cost_hist, t0_hist, t1_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_hist, t0_hist, t1_hist = gradient_descent(x, y, iterations=250, learning_rate=0.0003, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Model and Training Evaluation\n",
    "Now visualize the training process by plotting the `cost_hist` as a curve. Also, create a plot that shows the decision boundary of your final hypothesis (model) inside your data. Your plots should look like:\n",
    "\n",
    "![internet connection needed](https://gitlab.com/deep.TEACHING/educational-materials/raw/dev/media/voigt/images/Simple-Linear-Regression_Decision-Boundary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation_plt(cost_hist, theta_0, theta_1, x, y):\n",
    "    ''' Plots a cost curve and the decision boundary\n",
    "    \n",
    "    The Method plots a cost curve from a given training process (cost_hist). \n",
    "    It also plots the data set (x,y) and draws a linear decision boundary \n",
    "    with the parameters theta_0 and theta_1 into the plotted data set.\n",
    "    \n",
    "    Args:\n",
    "        cost_hist: vector, history of all cost values from a opitmization\n",
    "        theta_0: scalar, model parameter for boundary\n",
    "        theta_1: scalar, model parameter for boundary\n",
    "        x: vector, x values from the data set\n",
    "        y: vector, y values from the data set\n",
    "    '''\n",
    "    raise NotImplementedError(\"You should implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation_plt(cost_hist, theta_0, theta_1, x, y):\n",
    "    ''' Plots a cost curve and the decision boundary\n",
    "    \n",
    "    The Method plots a cost curve from a given training process (cost_hist). \n",
    "    It also plots the data set (x,y) and draws a linear decision boundary \n",
    "    with the parameters theta_0 and theta_1 into the plotted data set.\n",
    "    \n",
    "    Args:\n",
    "        cost_hist: vector, history of all cost values from a opitmization\n",
    "        theta_0: scalar, model parameter for boundary\n",
    "        theta_1: scalar, model parameter for boundary\n",
    "        x: vector, x values from the data set\n",
    "        y: vector, y values from the data set\n",
    "    '''\n",
    "    plt.figure(1, figsize=(17,7))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.subplots_adjust(wspace = 0.2)\n",
    "    plt.plot(np.linspace(0, len(cost_hist), len(cost_hist)), cost_hist)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Cost Curve')\n",
    "\n",
    "    x_ = np.arange(x.min() - 1., x.max() + 1., 0.1) \n",
    "    model = theta_0 + theta_1 * x_\n",
    "    plt.subplot(122)\n",
    "    plt.plot(x_, model, \"b-\")\n",
    "    plt.plot(x, y, \"rx\")\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Decision Boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation_plt(cost_hist, t0_hist[-1], t1_hist[-1], x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimize Hyperparameter\n",
    "In machine learning, hyperparameters are parameters whose values are set before starting the training process of the model. The learning rate is a hyperparameter, and it is a crucial parameter in the context of optimization with first-order methods in supervised learning. It can easily happen that your model does not learn if you have chosen an unsuited learning rate. To find a suitable learning rate for your problem, you need to try different ones. Implement a function `optimize_learning_rate` that trains your model with different learning rates and plots the different cost histories. Try to identify edge cases, e.g., cases when the learning rate is too high or too low, to develop a better feeling for the learning rate problem. Your plot could look like this:\n",
    "\n",
    "![internet connection needed](https://gitlab.com/deep.TEACHING/educational-materials/raw/dev/media/voigt/images/Simple-Linear-Regression_Learning-Rates.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_learning_rate(learning_rates, x, y):\n",
    "    ''' Train a model with diffrent learning rates and plots the costs\n",
    "    \n",
    "    Args:\n",
    "        learning_rates: vector, learning rates used to train a linear model\n",
    "        x: vector, x values from the data set\n",
    "        y: vector, y values from the data set\n",
    "    '''    \n",
    "    raise NotImplementedError(\"You should implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_learning_rate(learning_rates, x, y):\n",
    "    ''' Train a model with diffrent learning rates and plots the costs\n",
    "    \n",
    "    Args:\n",
    "        learning_rates: vector, learning rates used to train a linear model\n",
    "        x: vector, x values from the data set\n",
    "        y: vector, y values from the data set\n",
    "    '''    \n",
    "    plt.figure(figsize=(17,9))\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        cost_hist, t0_hist, t1_hist = gradient_descent(x, y, iterations=250, learning_rate=lr)\n",
    "        plt.plot(np.array(range(cost_hist.shape[0])), cost_hist, label='learning rate ' + str(lr))\n",
    "\n",
    "\n",
    "    plt.axis('auto')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Costs')\n",
    "    plt.title('Cost Curves')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "potential_lr = np.array([0.0001, 0.0007, 0.001, 0.007, .01, .0588, .05899])\n",
    "optimize_learning_rate(potential_lr, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary and Outlook\n",
    "\n",
    "During this exercise, fundamental elements of Machine Learning were covered. You should be able to answer the following questions:\n",
    "- What is a model using the example of a linear function as a hypothesis?\n",
    "- How do you quantify a model?\n",
    "- What is the gradient descent algorithm and what is its used for in the context of Machine Learning?\n",
    "- Can you explain the concept of hyperparameters and name some?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "Exercise: Simple Linear Regression <br/>\n",
    "by Christian Herta, Benjamin Voigt, Klaus Strohmenger <br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Christian Herta, Benjamin Voigt, Klaus Strohmenger\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
